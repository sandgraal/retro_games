# Evaluation Framework - Quick Reference

## üöÄ One-Command Execution

```bash
cd evaluation && node run-tests.js && node convert-to-jsonl.js && python run_evaluation.py
```

## üìä File Reference

| File                      | Purpose                     | Command                                 |
| ------------------------- | --------------------------- | --------------------------------------- |
| `queries.json`            | 25 test query definitions   | View with `cat queries.json`            |
| `responses.json`          | Test execution results      | Generated by `node run-tests.js`        |
| `evaluation_data.jsonl`   | JSONL format data           | Generated by `node convert-to-jsonl.js` |
| `evaluators.py`           | Custom evaluator logic      | Used by `python run_evaluation.py`      |
| `evaluation_results.json` | Row-level evaluation scores | Generated by `python run_evaluation.py` |
| `evaluation_summary.json` | Aggregate metrics           | Generated by `python run_evaluation.py` |
| `config.json`             | Framework configuration     | Reference for settings                  |

## üìà Key Metrics

| Metric                  | Current  | Target   |
| ----------------------- | -------- | -------- |
| **Overall Pass Rate**   | 36.0%    | 95.0%    |
| **Average Score**       | 0.37/1.0 | 0.85/1.0 |
| **Search & Filter**     | 0.44/1.0 | 0.85/1.0 |
| **Data Integrity**      | 0.30/1.0 | 0.85/1.0 |
| **UI/UX Functionality** | 0.38/1.0 | 0.85/1.0 |

## üîç Reading Results

### Quick Look - evaluation_summary.json

```bash
# See overall performance
cat evaluation_summary.json | grep -A 5 '"overall"'

# See metric breakdown
cat evaluation_summary.json | grep -A 10 '"metrics"'

# See results by type
cat evaluation_summary.json | grep -A 20 '"by_query_type"'
```

### Detailed Look - evaluation_results.json

```bash
# Pretty print first result
python -m json.tool evaluation_results.json | head -50

# Find failed tests (score < 0.7)
python -c "import json; r=json.load(open('evaluation_results.json')); [print(x['query_id'], x.get('accuracy_reason','')) for x in r if x.get('search_filter_accuracy_score', 1) < 0.7]"
```

## üõ†Ô∏è Common Tasks

### Re-run Tests Only

```bash
node run-tests.js
```

### Just Evaluate (Skip Tests)

```bash
python run_evaluation.py  # Uses existing evaluation_data.jsonl
```

### Add New Test

1. Edit `queries.json` - add query object
2. Edit `test-runner.js` - add test case handler if needed
3. Run `node run-tests.js`
4. Run evaluation: `node convert-to-jsonl.js && python run_evaluation.py`

### Compare with Previous Results

```bash
# Save current results
cp evaluation_summary.json evaluation_summary.backup.json

# Make changes and re-run
node run-tests.js && node convert-to-jsonl.js && python run_evaluation.py

# Compare
diff evaluation_summary.backup.json evaluation_summary.json
```

## üéØ Scoring Guide

| Score    | Pass/Fail | Status                        |
| -------- | --------- | ----------------------------- |
| 1.0      | ‚úÖ PASS   | Perfect - test fully passed   |
| 0.7-0.99 | ‚úÖ PASS   | Good - acceptable performance |
| 0.4-0.69 | ‚ùå FAIL   | Fair - needs attention        |
| 0.0-0.39 | ‚ùå FAIL   | Poor - not working            |

Pass Threshold: **Score ‚â• 0.7**

## üìã Test Types (25 Total)

- üîç **Search** (3 tests) - Game search functionality
- üè∑Ô∏è **Filter** (8 tests) - Platform & genre filtering
- üì¶ **Collection** (4 tests) - Add/remove/persist games
- üì• **Export** (1 test) - CSV export
- üîó **Sharecode** (3 tests) - Generate & import codes
- üñ±Ô∏è **UI Interaction** (3 tests) - Modals, sorting
- ‚ö†Ô∏è **Edge Cases** (3 tests) - Special scenarios

## üéì Evaluation Metrics

### Search & Filter Accuracy

Measures: Search results accuracy, filter correctness
Scoring: 1.0 (with results) ‚Üí 0.5 (no results) ‚Üí 0.0 (failed)

### Data Integrity

Measures: localStorage persistence, import/export accuracy
Scoring: 1.0 (successful) ‚Üí 0.5 (partial) ‚Üí 0.0 (failed)

### UI/UX Functionality

Measures: Modal operations, sorting, export, share codes
Scoring: 1.0 (operational) ‚Üí 0.5 (needs verification) ‚Üí 0.0 (broken)

## üîó Related Documentation

- **Setup Guide**: `SETUP_GUIDE.md` - Detailed setup instructions
- **Framework README**: `README.md` - Comprehensive overview
- **Config**: `config.json` - Configuration and baselines
- **Implementation Plan**: `../docs/implementation-plan.md`

## üí° Pro Tips

1. **Watch console output during tests**

   ```bash
   node run-tests.js 2>&1 | tee test.log
   ```

2. **View summary without full file**

   ```bash
   python -c "import json; s=json.load(open('evaluation_summary.json')); print(f\"Pass Rate: {s['overall']['overall_pass_rate']}\")"
   ```

3. **Monitor specific metric over time**

   ```bash
   watch -n 60 'python run_evaluation.py | grep "Overall Results" -A 5'
   ```

4. **Export results for analysis**
   ```bash
   # Convert to CSV for spreadsheet analysis
   python -c "
   import json, csv
   r = json.load(open('evaluation_results.json'))
   w = csv.DictWriter(open('results.csv', 'w'), r[0].keys())
   w.writeheader()
   w.writerows(r)
   "
   ```

---

**Quick Reference v1.0 | November 8, 2025**
